---
layout: post
title: "EKS EFK êµ¬ì¶•"
author: "Bys"
category: cloud
date: 2021-04-28 01:00:00
tags: eks efk fluentd elasticsearch kibana
---

#### FluentD ì„¤ì •  
`Create Configmap cluster-info`
```bash
kubectl create configmap cluster-info \
--from-literal=cluster.name=MyClusterName
--from-literal=logs.region=ap-northeast-2 -n amazon-cloudwatch
```
<br>

`Create Namespace`
```bash
kubectl apply -f https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/cloudwatch-namespace.yaml
```
<br>


`Fluent DaemonSet`
```bash
kubectl apply -f https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/fluentd/fluentd.yaml
```
<br>

ì´ë ‡ê²Œ ë°°í¬ë¥¼ í•˜ë©´ amazon-cloudwatch namespaceìƒì— fluentd-cloudwatch-* í˜•íƒœì˜ podê°€ eks clusterì— ë°°í¬ê°€ ëœë‹¤.  
ë°°í¬ê°€ ëœ Podì˜ Containerë¡œê·¸ëŠ” Cloudwatch Log Groupsì—ì„œ í™•ì¸ í•  ìˆ˜ ìˆë‹¤.  

#### AWS ElasticSearch ì„¤ì •  

`ElasticSearch`  
AWSì—ì„œ ESìƒì„±ì„ í•˜ê²Œ ë˜ë©´ ê¸°ë³¸ì ì¸ ê°’ë“¤ì„ ì…‹íŒ…í•˜ê³  Fine-grained access controlì„ í™œì„±í™” ì‹œì¼œ ê¶Œí•œì„ Control í•œë‹¤.  
Action -> Modify authenticationì— ARN ì…‹íŒ…ê³¼ Master user ìƒì„±ì´ ê°€ëŠ¥í•˜ë©°, Master UserëŠ” Kibana í™”ë©´ì˜ Master Userê°€ ëœë‹¤.  

FluentDê°€ ìˆ˜ì§‘í•œ ë¡œê·¸ë¥¼ ElasticSearchë¡œ ë³´ë‚´ê¸° ìœ„í•´ì„œëŠ” Log Groupsì—ì„œ Subscription filtersì˜ Create - Create Elasticsearch subscription filterì„ ëˆŒëŸ¬ ì„¤ì •í•œë‹¤.  
(Log Groups -> Subscription filters -> Create -> Create Elasticsearch subscription filter)   

í•´ë‹¹ ì„¤ì •ì´ ì™„ë£Œë˜ë©´ Lambda Functionì´ ìë™ìœ¼ë¡œ ìƒì„±ë˜ë©° Cloudwatch ì—ì„œ Lambdaë¥¼ í†µí•´ ElasticSearchë¡œ ë°ì´í„°ê°€ ë“¤ì–´ê°„ë‹¤.  

Lambda Functionì˜ ì†ŒìŠ¤ë¥¼ ë³´ë©´ ë°ì´í„° Indexë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ë³´ë‚¸ë‹¤. (cwl-*)  
CloudWatch ë¡œê·¸ ê·¸ë£¹ì„ ë™ì¼í•œ Amazon ES ë„ë©”ì¸ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•˜ê¸° ìœ„í•´ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ indexNmaeì„ ê·¸ë£¹ë³„ë¡œ ë³´ë‚¼ ìˆ˜ ìˆë„ë¡ ì„¤ì •í•œë‹¤.  
```javascript
var indexName = [
        'cwl-' + payload.logGroup.substring(payload.logGroup.lastIndexOf('/') + 1) + '-' + timestamp.getUTCFullYear(),
        ('0' + (timestamp.getUTCMonth() + 1)).slice(-2),
        ('0' + timestamp.getUTCDate()).slice(-2) 
        ].join('.');
```

### ì°¸ê³   
`Lambda function ìˆ˜ì • ë²„ì „`  
```javascript
// v1.1.2
var https = require('https');
var zlib = require('zlib');
var crypto = require('crypto');

var endpoint = '<aws es endpoint>';

exports.handler = function(input, context) {
    // decode input from base64
    var zippedInput = new Buffer(input.awslogs.data, 'base64');

    // decompress the input
    zlib.gunzip(zippedInput, function(error, buffer) {
        if (error) { context.fail(error); return; }

        // parse the input from JSON
        var awslogsData = JSON.parse(buffer.toString('utf8'));

        // transform the input to Elasticsearch documents
        var elasticsearchBulkData = transform(awslogsData);

        // skip control messages
        if (!elasticsearchBulkData) {
            console.log('Received a control message');
            context.succeed('Control message handled successfully');
            return;
        }

        // post documents to the Amazon Elasticsearch Service
        post(elasticsearchBulkData, function(error, success, statusCode, failedItems) {
            // ìˆ˜ì •ì™„ë£Œ
            console.log('Success: ' + JSON.stringify(failedItems));
            console.log('Response: ' + JSON.stringify({ 
                "statusCode": statusCode 
            }));

            if (error) { 
                console.log('Error: ' + JSON.stringify(error, null, 2));

                if (failedItems && failedItems.length > 0) {
                    console.log("Failed Items: " +
                        JSON.stringify(failedItems, null, 2));
                }

                context.fail(JSON.stringify(error));
            } else {
                console.log('Success: ' + JSON.stringify(success));
                context.succeed('Success');
            }
        });
    });
};

function transform(payload) {
    if (payload.messageType === 'CONTROL_MESSAGE') {
        return null;
    }

    var bulkRequestBody = '';

    payload.logEvents.forEach(function(logEvent) {
        var timestamp = new Date(1 * logEvent.timestamp);

        // index name format: cwl-YYYY.MM.DD
        var namespace = payload.logGroup.toLowerCase().split('/')[5]
        var apiname = payload.logGroup.toLowerCase().split('/')[6]
        var indexName = [
            'mydata-' + namespace + '-' + apiname + '-' + timestamp.getUTCFullYear(),
            ('0' + (timestamp.getUTCMonth() + 1)).slice(-2),  // month
            ('0' + timestamp.getUTCDate()).slice(-2)          // day
        ].join('.');

        var source = buildSource(logEvent.message, logEvent.extractedFields);
        source['@id'] = logEvent.id;
        source['@timestamp'] = new Date(1 * logEvent.timestamp).toISOString();
        source['@message'] = logEvent.message;
        source['@owner'] = payload.owner;
        source['@log_group'] = payload.logGroup;
        source['@log_stream'] = payload.logStream;

        var action = { "index": {} };
        action.index._index = indexName;
        action.index._type = payload.logGroup;
        action.index._id = logEvent.id;
        
        bulkRequestBody += [ 
            JSON.stringify(action), 
            JSON.stringify(source),
        ].join('\n') + '\n';
    });
    return bulkRequestBody;
}

function buildSource(message, extractedFields) {
    if (extractedFields) {
        var source = {};

        for (var key in extractedFields) {
            if (extractedFields.hasOwnProperty(key) && extractedFields[key]) {
                var value = extractedFields[key];

                if (isNumeric(value)) {
                    source[key] = 1 * value;
                    continue;
                }

                jsonSubString = extractJson(value);
                if (jsonSubString !== null) {
                    source['$' + key] = JSON.parse(jsonSubString);
                }

                source[key] = value;
            }
        }
        return source;
    }

    jsonSubString = extractJson(message);
    if (jsonSubString !== null) { 
        return JSON.parse(jsonSubString); 
    }

    return {};
}

function extractJson(message) {
    var jsonStart = message.indexOf('{');
    if (jsonStart < 0) return null;
    var jsonSubString = message.substring(jsonStart);
    return isValidJson(jsonSubString) ? jsonSubString : null;
}

function isValidJson(message) {
    try {
        JSON.parse(message);
    } catch (e) { return false; }
    return true;
}

function isNumeric(n) {
    return !isNaN(parseFloat(n)) && isFinite(n);
}

function post(body, callback) {
    var requestParams = buildRequest(endpoint, body);

    var request = https.request(requestParams, function(response) {
        var responseBody = '';
        response.on('data', function(chunk) {
            responseBody += chunk;
        });
        response.on('end', function() {
            var info = JSON.parse(responseBody);
            var failedItems;
            var success;
            
            if (response.statusCode >= 200 && response.statusCode < 299) {
                failedItems = info.items.filter(function(x) {
                    return x.index.status >= 300;
                });

                success = { 
                    "attemptedItems": info.items.length,
                    "successfulItems": info.items.length - failedItems.length,
                    "failedItems": failedItems.length
                };
            }

            var error = response.statusCode !== 200 || info.errors === true ? {
                "statusCode": response.statusCode,
                "responseBody": responseBody
            } : null;

            callback(error, success, response.statusCode, failedItems);
        });
    }).on('error', function(e) {
        callback(e);
    });
    request.end(requestParams.body);
}

function buildRequest(endpoint, body) {
    var endpointParts = endpoint.match(/^([^\.]+)\.?([^\.]*)\.?([^\.]*)\.amazonaws\.com$/);
    var region = endpointParts[2];
    var service = endpointParts[3];
    var datetime = (new Date()).toISOString().replace(/[:\-]|\.\d{3}/g, '');
    var date = datetime.substr(0, 8);
    var kDate = hmac('AWS4' + process.env.AWS_SECRET_ACCESS_KEY, date);
    var kRegion = hmac(kDate, region);
    var kService = hmac(kRegion, service);
    var kSigning = hmac(kService, 'aws4_request');
    
    var request = {
        host: endpoint,
        method: 'POST',
        path: '/_bulk',
        body: body,
        headers: { 
            'Content-Type': 'application/json',
            'Host': endpoint,
            'Content-Length': Buffer.byteLength(body),
            'X-Amz-Security-Token': process.env.AWS_SESSION_TOKEN,
            'X-Amz-Date': datetime
        }
    };

    var canonicalHeaders = Object.keys(request.headers)
        .sort(function(a, b) { return a.toLowerCase() < b.toLowerCase() ? -1 : 1; })
        .map(function(k) { return k.toLowerCase() + ':' + request.headers[k]; })
        .join('\n');

    var signedHeaders = Object.keys(request.headers)
        .map(function(k) { return k.toLowerCase(); })
        .sort()
        .join(';');

    var canonicalString = [
        request.method,
        request.path, '',
        canonicalHeaders, '',
        signedHeaders,
        hash(request.body, 'hex'),
    ].join('\n');

    var credentialString = [ date, region, service, 'aws4_request' ].join('/');

    var stringToSign = [
        'AWS4-HMAC-SHA256',
        datetime,
        credentialString,
        hash(canonicalString, 'hex')
    ] .join('\n');

    request.headers.Authorization = [
        'AWS4-HMAC-SHA256 Credential=' + process.env.AWS_ACCESS_KEY_ID + '/' + credentialString,
        'SignedHeaders=' + signedHeaders,
        'Signature=' + hmac(kSigning, stringToSign, 'hex')
    ].join(', ');

    return request;
}

function hmac(key, str, encoding) {
    return crypto.createHmac('sha256', key).update(str, 'utf8').digest(encoding);
}

function hash(str, encoding) {
    return crypto.createHash('sha256').update(str, 'utf8').digest(encoding);
}

function logFailure(error, failedItems) {
    if (logFailedResponses){
      console.log('Error: ' + JSON.stringify(error,null, 2));

      if (failedItems && failedItems.length > 0) {
        console.log("Failed Items: " + JSON.stringify(failedItems, null, 2));
      }
    }
}
```


Elastic Searchì˜ SGì—ëŠ” Lambda Functionì˜ Inboundë¥¼ í—ˆìš©í•´ì£¼ì–´ì•¼ í•˜ëŠ”ë° í•´ë‹¹ ëŒ€ì—­ì€ ElasticSearchê°€ ìƒì„±ëœ Subnetì˜ 80/443í¬íŠ¸ë¥¼ ì¡ì•„ì£¼ë©´ ëœë‹¤.  

`Kibana`  
KibanaëŠ” AWS ESë¥¼ ìƒì„±í•  ê²½ìš° Pluginìœ¼ë¡œ ì œê³µí•´ì£¼ê³  ìˆìœ¼ë©° ElasticSearchì˜ VPC endpoint ì£¼ì†Œì˜ /_plugin/kibana/ ë¥¼ ë¶™ì—¬ì„œ ì ‘ì† ê°€ëŠ¥í•˜ë‹¤.  
Discover ë©”ë‰´ì˜ CreateIndexPatternë²„íŠ¼ì„ ëˆŒëŸ¬ Index Patternì„ ì¡ì•„ì¤€ë‹¤.  Index Patternì€ ìœ„ Lambda Functionì—ì„œ ë³´ë‚´ì£¼ëŠ” Index Pattern(cwl-*)ì„ ì„¤ì •í•œë‹¤.  

Userì™€ ê¶Œí•œ ìƒì„±ì„ ìœ„í•´ì„œëŠ” ë©”ë‰´ -> Security -> Internal User/Role ì„¤ì •ì´ í•„ìš”í•˜ë‹¤.  
<br>


íì‡„ë§ í™˜ê²½ì´ë¼ë©´ ì•„ë˜ yamlíŒŒì¼ì—ì„œ image ë³€ê²½ì´ í•„ìš”í•˜ë‹¤. 
```yaml
spec:
  serviceAccountName: fluentd
  terminationGracePeriodSeconds: 30
  # Because the image's entrypoint requires to write on /fluentd/etc but we mount configmap there which is read-only,
  # this initContainers workaround or other is needed.
  # See https://github.com/fluent/fluentd-kubernetes-daemonset/issues/90
  initContainers:
    - name: copy-fluentd-config
      # ìˆ˜ì •í•„ìš”
      image: busybox
      command: ['sh', '-c', 'cp /config-volume/..data/* /fluentd/etc']
      volumeMounts:
        - name: config-volume
          mountPath: /config-volume
        - name: fluentdconf
          mountPath: /fluentd/etc
    - name: update-log-driver
      # ìˆ˜ì •í•„ìš”
      image: busybox
      command: ['sh','-c','']
  containers:
    - name: fluentd-cloudwatch
      # ìˆ˜ì •í•„ìš”
      image: fluent/fluentd-kubernetes-daemonset:v1.7.3-debian-cloudwatch-1.0
```
<br>

EFKí™˜ê²½ì—ì„œ Kibanaìª½ì— ë¡œê·¸ ìˆœì„œê°€ ë’¤ì„ì—¬ ë‚˜ì˜¤ëŠ” í˜„ìƒì´ ìˆì—ˆëŠ”ë° fluentd.yaml íŒŒì¼ì—ì„œ multiline_start_regexp ì„¤ì •ì„ ë³€ê²½í•˜ì—¬ ì¡°ì •í•˜ì˜€ë‹¤.  
Log íŒ¨í„´ì„ ì •ê·œ í‘œí˜„ì‹ì— ë§ì¶”ì–´ í•˜ë‚˜ì˜ ë¡œê·¸ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì–´ Clodwatchë¡œ ì „ì†¡  
```bash
11:09:04[301] logdata~~~~~~
(T) 11:09:04[301] logdata~~~~~~
logdata~~~~~~
11:09:04[301] logdata~~~~~~
```
<br>

```ruby
<label @containers>
......
<filter **>
  @type concat
  key log
  # ìˆ˜ì •ì™„ë£Œ
  multiline_start_regexp /\((D|E|T|I)\)\s\d{2}[:]\d{2}[:]\d{2}\[\d{3}\]/
  #multiline_start_regexp /^(date:){0,1}\d{4}[-/]\d{1,2}[-/]\d{1,2}/
  separator ""
  flush_interval 5
  timeout_label @NORMAL
</filter>
```
<br>


Kibana Indexê°€ ì§€ì†ì ìœ¼ë¡œ ìŒ“ì„ì— ë”°ë¼ ê´€ë¦¬ì˜ í•„ìš”ì„±ì´ ìƒê²¨ Policyë¥¼ ì ìš©í•˜ì—¬ ê´€ë¦¬ë¥¼ í•˜ì˜€ë‹¤.  
ì•„ë˜ì™€ ê°™ì´ Delete Policyë¥¼ ë§Œë“¤ì–´ 30ì¼ì´ Indexì˜ ê²½ìš° Hot -> Deleteë¡œ ë³´ë‚´ëŠ” ì •ì±…ì„ ë§Œë“¤ì–´ ì‚¬ìš©í•˜ì˜€ë‹¤.  
Indexê°€ ìƒì„±ë˜ëŠ” íŒ¨í„´ì€ IndexName-yyyy.mm.dd íŒ¨í„´ì´ì—ˆê¸° ë•Œë¬¸ì—, ism_templateìœ¼ë¡œ index_patternsë¥¼ ì¡ì•„ì£¼ë©´ ì‹ ê·œë¡œ ìƒì„±ë˜ëŠ” indexì—ë„ í•´ë‹¹ ì •ì±…ì´ ë°”ë¡œ ì ìš©ì´ ëœë‹¤.  
`policy`
```json
{
    "policy": {
        "policy_id": "delete_policy",
        "description": "Demonstrate a hot-delete workflow.",
        "last_updated_time": 1625028517692,
        "schema_version": 1,
        "error_notification": null,
        "default_state": "hot",
        "states": [
            {
                "name": "hot",
                "actions": [],
                "transitions": [
                    {
                        "state_name": "delete",
                        "conditions": {
                            "min_index_age": "30d"
                        }
                    }
                ]
            },
            {
                "name": "delete",
                "actions": [
                    {
                        "delete": {}
                    }
                ],
                "transitions": []
            }
        ],
        "ism_template": [
            {
                "index_patterns": [
                    "mydata-*"
                ],
                "priority": 10
            }
        ]
    }
}
```

<br>

### ì°¸ê³   
`fluentd.yaml ìˆ˜ì • ë²„ì „`  
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd
  namespace: amazon-cloudwatch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluentd-role
rules:
  - apiGroups: [""]
    resources:
      - namespaces
      - pods
      - pods/logs
    verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluentd-role-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: fluentd-role
subjects:
  - kind: ServiceAccount
    name: fluentd
    namespace: amazon-cloudwatch
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: amazon-cloudwatch
  labels:
    k8s-app: fluentd-cloudwatch
data:
  fluent.conf: |
    @include containers.conf
    @include systemd.conf
    @include host.conf

    <match fluent.**>
      @type null
    </match>
  containers.conf: |
    <source>
      @type tail
      @id in_tail_container_logs
      @label @containers
      path /var/log/containers/*.log
      exclude_path ["/var/log/containers/cloudwatch-agent*", "/var/log/containers/fluentd*"]
      pos_file /var/log/fluentd-containers.log.pos
      tag *
      read_from_head true
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_cwagent_logs
      @label @cwagentlogs
      path /var/log/containers/cloudwatch-agent*
      pos_file /var/log/cloudwatch-agent.log.pos
      tag *
      read_from_head true
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_fluentd_logs
      @label @fluentdlogs
      path /var/log/containers/fluentd*
      pos_file /var/log/fluentd.log.pos
      tag *
      read_from_head true
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>

    <label @fluentdlogs>
      <filter **>
        @type kubernetes_metadata
        @id filter_kube_metadata_fluentd
      </filter>

      <filter **>
        @type record_transformer
        @id filter_fluentd_stream_transformer
        <record>
          stream_name ${tag_parts[3]}
          group_name "/aws/containerinsights/#{ENV.fetch('CLUSTER_NAME')}/application"
        </record>
      </filter>

      <match **>
        @type relabel
        @label @NORMAL
      </match>
    </label>

    <label @containers>
      <filter **>
        @type kubernetes_metadata
        @id filter_kube_metadata
      </filter>

      <filter **>
        @type record_transformer
        @id filter_containers_stream_transformer
        enable_ruby
        remove_keys $.docker, $.kubernetes.container_name, $.kubernetes.namespace_name, $.kubernetes.container_image, $.kubernetes.container_image_id, $.kubernetes.pod_id, $.kubernetes.labels, $.kubernetes.master_url, $.kubernetes.namespace_id
        <record>
          stream_name ${tag_parts[3]}
          group_name "/aws/containerinsights/#{ENV.fetch('CLUSTER_NAME')}/application/${record['kubernetes']['namespace_name']}/${record['kubernetes']['container_name']}"
        </record>
      </filter>

      <filter **>
        @type concat
        key log
        multiline_start_regexp /\((D|E|T|I)\)\s\d{2}[:]\d{2}[:]\d{2}\[\d{3}\]/
        separator ""
        flush_interval 5
        timeout_label @NORMAL
      </filter>

      <match **>
        @type relabel
        @label @NORMAL
      </match>
    </label>

    <label @cwagentlogs>
      <filter **>
        @type kubernetes_metadata
        @id filter_kube_metadata_cwagent
      </filter>

      <filter **>
        @type record_transformer
        @id filter_cwagent_stream_transformer
        <record>
          stream_name ${tag_parts[3]}
          group_name "/aws/containerinsights/#{ENV.fetch('CLUSTER_NAME')}/application"
        </record>
      </filter>

      <filter **>
        @type concat
        key log
        multiline_start_regexp /^(date:){0,1}\d{4}[-/]\d{1,2}[-/]\d{1,2}/
        separator ""
        flush_interval 5
        timeout_label @NORMAL
      </filter>

      <match **>
        @type relabel
        @label @NORMAL
      </match>
    </label>

    <label @NORMAL>
      <match **>
        @type cloudwatch_logs
        @id out_cloudwatch_logs_containers
        region "#{ENV.fetch('REGION')}"
        log_group_name_key group_name
        remove_log_group_name_key false
        log_stream_name_key stream_name
        remove_log_stream_name_key true
        auto_create_stream true
        <buffer>
          flush_interval 5
          chunk_limit_size 2m
          queued_chunks_limit_size 32
          retry_forever true
        </buffer>
      </match>
    </label>
  systemd.conf: |
    <source>
      @type systemd
      @id in_systemd_kubelet
      @label @systemd
      filters [{ "_SYSTEMD_UNIT": "kubelet.service" }]
      <entry>
        field_map {"MESSAGE": "message", "_HOSTNAME": "hostname", "_SYSTEMD_UNIT": "systemd_unit"}
        field_map_strict true
      </entry>
      path /var/log/journal
      <storage>
        @type local
        persistent true
        path /var/log/fluentd-journald-kubelet-pos.json
      </storage>
      read_from_head true
      tag kubelet.service
    </source>

    <source>
      @type systemd
      @id in_systemd_kubeproxy
      @label @systemd
      filters [{ "_SYSTEMD_UNIT": "kubeproxy.service" }]
      <entry>
        field_map {"MESSAGE": "message", "_HOSTNAME": "hostname", "_SYSTEMD_UNIT": "systemd_unit"}
        field_map_strict true
      </entry>
      path /var/log/journal
      <storage>
        @type local
        persistent true
        path /var/log/fluentd-journald-kubeproxy-pos.json
      </storage>
      read_from_head true
      tag kubeproxy.service
    </source>

    <source>
      @type systemd
      @id in_systemd_docker
      @label @systemd
      filters [{ "_SYSTEMD_UNIT": "docker.service" }]
      <entry>
        field_map {"MESSAGE": "message", "_HOSTNAME": "hostname", "_SYSTEMD_UNIT": "systemd_unit"}
        field_map_strict true
      </entry>
      path /var/log/journal
      <storage>
        @type local
        persistent true
        path /var/log/fluentd-journald-docker-pos.json
      </storage>
      read_from_head true
      tag docker.service
    </source>

    <label @systemd>
      <filter **>
        @type kubernetes_metadata
        @id filter_kube_metadata_systemd
      </filter>

      <filter **>
        @type record_transformer
        @id filter_systemd_stream_transformer
        <record>
          stream_name ${tag}-${record["hostname"]}
          group_name "/aws/containerinsights/#{ENV.fetch('CLUSTER_NAME')}/dataplane"
        </record>
      </filter>

      <match **>
        @type cloudwatch_logs
        @id out_cloudwatch_logs_systemd
        region "#{ENV.fetch('REGION')}"
        log_group_name_key group_name
        remove_log_group_name_key false
        log_stream_name_key stream_name
        auto_create_stream true
        remove_log_stream_name_key true
        <buffer>
          flush_interval 5
          chunk_limit_size 2m
          queued_chunks_limit_size 32
          retry_forever true
        </buffer>
      </match>
    </label>
  host.conf: |
    <source>
      @type tail
      @id in_tail_dmesg
      @label @hostlogs
      path /var/log/dmesg
      pos_file /var/log/dmesg.log.pos
      tag host.dmesg
      read_from_head true
      <parse>
        @type syslog
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_secure
      @label @hostlogs
      path /var/log/secure
      pos_file /var/log/secure.log.pos
      tag host.secure
      read_from_head true
      <parse>
        @type syslog
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_messages
      @label @hostlogs
      path /var/log/messages
      pos_file /var/log/messages.log.pos
      tag host.messages
      read_from_head true
      <parse>
        @type syslog
      </parse>
    </source>

    <label @hostlogs>
      <filter **>
        @type kubernetes_metadata
        @id filter_kube_metadata_host
      </filter>

      <filter **>
        @type record_transformer
        @id filter_containers_stream_transformer_host
        <record>
          stream_name ${tag}-${record["host"]}
          group_name "/aws/containerinsights/#{ENV.fetch('CLUSTER_NAME')}/host"
        </record>
      </filter>

      <match host.**>
        @type cloudwatch_logs
        @id out_cloudwatch_logs_host_logs
        region "#{ENV.fetch('REGION')}"
        log_group_name_key group_name
        remove_log_group_name_key false
        log_stream_name_key stream_name
        remove_log_stream_name_key true
        auto_create_stream true
        <buffer>
          flush_interval 5
          chunk_limit_size 2m
          queued_chunks_limit_size 32
          retry_forever true
        </buffer>
      </match>
    </label>
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-cloudwatch
  namespace: amazon-cloudwatch
spec:
  selector:
    matchLabels:
      k8s-app: fluentd-cloudwatch
  template:
    metadata:
      labels:
        k8s-app: fluentd-cloudwatch
      annotations:
        configHash: 8915de4cf9c3551a8dc74c0137a3e83569d28c71044b0359c2578d2e0461825
    spec:
      serviceAccountName: fluentd
      terminationGracePeriodSeconds: 30
      # Because the image's entrypoint requires to write on /fluentd/etc but we mount configmap there which is read-only,
      # this initContainers workaround or other is needed.
      # See https://github.com/fluent/fluentd-kubernetes-daemonset/issues/90
      initContainers:
        - name: copy-fluentd-config
          image: busybox
          command: ['sh', '-c', 'cp /config-volume/..data/* /fluentd/etc']
          volumeMounts:
            - name: config-volume
              mountPath: /config-volume
            - name: fluentdconf
              mountPath: /fluentd/etc
        - name: update-log-driver
          image: busybox
          command: ['sh','-c','']
      containers:
        - name: fluentd-cloudwatch
          image: fluent/fluentd-kubernetes-daemonset:v1.7.3-debian-cloudwatch-1.0
          env:
            - name: REGION
              valueFrom:
                configMapKeyRef:
                  name: cluster-info
                  key: logs.region
            - name: CLUSTER_NAME
              valueFrom:
                configMapKeyRef:
                  name: cluster-info
                  key: cluster.name
            - name: CI_VERSION
              value: "k8s/1.1.0"
          resources:
            limits:
              memory: 400Mi
            requests:
              cpu: 100m
              memory: 200Mi
          volumeMounts:
            - name: config-volume
              mountPath: /config-volume
            - name: fluentdconf
              mountPath: /fluentd/etc
            - name: varlog
              mountPath: /var/log
            - name: varlibdockercontainers
              mountPath: /var/lib/docker/containers
              readOnly: true
            - name: runlogjournal
              mountPath: /run/log/journal
              readOnly: true
            - name: dmesg
              mountPath: /var/log/dmesg
              readOnly: true
      volumes:
        - name: config-volume
          configMap:
            name: fluentd-config
        - name: fluentdconf
          emptyDir: {}
        - name: varlog
          hostPath:
            path: /var/log
        - name: varlibdockercontainers
          hostPath:
            path: /var/lib/docker/containers
        - name: runlogjournal
          hostPath:
            path: /run/log/journal
        - name: dmesg
          hostPath:
            path: /var/log/dmesg
```

---

## ğŸ“š References


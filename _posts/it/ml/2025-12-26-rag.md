---
layout: post
title: "RAG(Retrieval-Augmented Generation)ì„ í™œìš©í•œ Chatbot"
author: "Bys"
category: ml
date: 2025-12-26 01:00:00
keywords: "rag, ai, llm"
tags: rag ai chatbot llm
---

# [RAG](https://aws.amazon.com/what-is/retrieval-augmented-generation/)  
- AWS: Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response
- OpenAI: Retrieval Augmented Generation (RAG) is a technique that improves a modelâ€™s responses by injecting external context into its prompt at runtime.
- Google: RAG (Retrieval-Augmented Generation) is an AI framework that combines the strengths of traditional information retrieval systems (such as search and databases) with the capabilities of generative large language models (LLMs).

RAGëŠ” LLM ëª¨ë¸ì˜ ë‹µë³€ì„ ë” ì¢‹ê²Œ ë§Œë“¤ê¸° ìœ„í•´ì„œ ì™¸ë¶€ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ `ìœ ìš©í•œ ë°ì´í„°ë¥¼ ì¡°íšŒ(Retrieval)`í•˜ì—¬ `LLM ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ ê°•í™”ì‹œí‚¤ê³ (augmented)` `ë” ì¢‹ì€ ë‹µë³€(Generation)`ì„ ì œê³µí•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. 


## 1. RAG Architecture
AWS ë¬¸ì„œ[1]ì—ì„œ ì„¤ëª…í•˜ëŠ” RAG ë™ì‘ë°©ì‹ì— ë”°ë¥´ë©´ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ë‹¤.  
![rag.png](/assets/it/ml/rag/rag.png)

ê¸°ì¡´ì—ëŠ” ì‚¬ìš©ìê°€ ì‘ì„±í•œ ì¿¼ë¦¬ê°€ ê·¸ëŒ€ë¡œ LLM ëª¨ë¸ë¡œ ì „ë‹¬ë˜ì—ˆë‹¤ë©´, RAG ê¸°ë°˜ì—ì„œëŠ” ì™¸ë¶€ ë°ì´í„°ì†ŒìŠ¤(AWS: Knowledge Bases)ì— ê°€ì„œ ê´€ë ¨ëœ ë¬¸ì„œê°€ ìˆëŠ”ì§€ ë¨¼ì € ì¡°íšŒí•˜ì—¬ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ë‹´ì•„ LLM ëª¨ë¸ì— ì „ë‹¬í•œë‹¤. ì´ë¡œ ì¸í•´ LLM ëª¨ë¸ì€ ìì‹ ì´ ëª°ëë˜ ì •ë³´ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì…ë ¥ë°›ì•„ ë” ì¢‹ì€ ë‹µë³€ì„ í•  ìˆ˜ ìˆê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.  


![pre-processing.png](/assets/it/ml/rag/pre-processing.png)  
ë‹¨ê³„ì ìœ¼ë¡œ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë°ì´í„°ë¥¼ Vector í™” í•˜ëŠ” ì‚¬ì „ ì²˜ë¦¬ ë‹¨ê³„ë¥¼ í†µí•´ Vector DBì— ì €ì¥í•œë‹¤.  

![runtime-processing.png](/assets/it/ml/rag/runtime-processing.png)  
ì´í›„ Runtime ì‹œì—ëŠ” ë¹„ìŠ·í•œ Documentë¥¼ Vector DBì—ì„œ ì°¾ì•„ LLM ëª¨ë¸ì— Query + Relevant Information ì„ ì „ë‹¬í•˜ì—¬ LLM í™œìš©ì„ ê°•í™”í•˜ëŠ” ê²ƒì´ë‹¤. 


AWSì™€ ë¹„ìŠ·í•˜ê²Œ IBM ì—ì„œëŠ” RAG Architecture ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ê¸°ë³¸ ì»¨ì…‰ì„ ì œê³µí•œë‹¤[2].  
![rag-architecture.png](/assets/it/ml/rag/rag-architecture.png)  

AI Engineer ëŠ” ì‚¬ì „ì— ë°ì´í„° ì²˜ë¦¬ë¥¼ ì§„í–‰í•´ì•¼í•œë‹¤. ë¹„ì •í˜• ë°ì´í„°ì— ëŒ€í•´ì„œ Vector Embeddingì„ í†µí•´ Vector DBì— ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³ , ì¶”í›„ ëª¨ë¸ì— Query ì‹œì— ê´€ë ¨ëœ ì •ë³´ë¥¼ Vector DBì—ì„œ ì¡°íšŒí•˜ì—¬ LLMì— Query + Relevant Information ì„ ê°™ì´ ì „ë‹¬í•˜ëŠ” ê²ƒì´ë‹¤.  


## 2. Vector í™”



## 3. Test 
ë¡œì»¬ì—ì„œ Ollama ëª¨ë¸ì— RAG ë¥¼ í™œìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ í•´ ë³¼ ìˆ˜ ìˆë„ë¡ í•œë‹¤. 

- ì»¨ì…‰: ë‚´ê°€ í–ˆë˜ ì¼ì— ëŒ€í•´ RAGë¥¼ í†µí•´ ê°•í™”í•˜ê³ , linked, wanted ì»¤ë¦¬ì–´ ì´ë ¥ì„œë¥¼ í†µí•´ LLM ëª¨ë¸ì´ ë‚´ê°€ í–ˆë˜ì¼ì„ ê¸°ë°˜ìœ¼ë¡œ ë‚˜ì™€ ê°™ì´ ëŒ€ë‹µì„ í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. 
- êµ¬ì¡°:
  - ë‚´ê°€ í–ˆë˜ ì¼ì€ ëª¨ë‘ íŠ¹ì • ê²½ë¡œ(rag_path) í•˜ìœ„ì— markdown ì–¸ì–´ë¡œ ì €ì¥ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— .md íŒŒì¼ë§Œ ê²€ìƒ‰í•œë‹¤. 
  - ë‚˜ì— ëŒ€í•œ ì†Œê°œëŠ” ëª¨ë‘ /me/ í•˜ìœ„ì— linkedin.pdf, wanted.pdf, summary.txt íŒŒì¼ë¡œ ì €ì¥ëœë‹¤.  
  - 

ë¨¼ì €, í…ŒìŠ¤íŠ¸ë¥¼ í•˜ê¸° ìœ„í•œ í™˜ê²½ìœ¼ë¡œ Vector DBì™€ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ Vectorí™” í•  ìˆ˜ ìˆëŠ” Embedding Model ì´ í•„ìš”í•˜ë‹¤. ì´ ë²ˆ í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì€ í™˜ê²½ì„ ì‚¬ìš©í•œë‹¤.  
- VectorDB: [Chroma](https://www.trychroma.com/home)
- Embedding Model: [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)


---

```python
from dotenv import load_dotenv
from openai import OpenAI
from pypdf import PdfReader
import gradio as gr
import chromadb
from chromadb.utils import embedding_functions

print("âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ")
```


`Ollama ì…‹íŒ…`  
```python
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(override=True)

# OpenAI í´ë¼ì´ì–¸íŠ¸ (Ollama ì‚¬ìš©)
openai = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama"
)
model_name = "gpt-oss:20b-cloud"

print(f"âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì™„ë£Œ (ëª¨ë¸: {model_name})")
```



`Linkedin, Wanted ë“±ì— ì €ì¥ëœ PDF íŒŒì¼ì„ ë‹¤ìš´ ë°›ì•„ /me/ í•˜ìœ„ í´ë”ì— ì €ì¥í•œ í›„ ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•œë‹¤. summary.txt ëŠ” ìì‹ ì— ëŒ€í•œ ê°„ë‹¨í•œ ì†Œê°œì„œë‹¤.`  
```python
# LinkedIn PDF ì½ê¸°
reader = PdfReader("../me/linkedin.pdf")
linkedin = ""
for page in reader.pages:
    text = page.extract_text()
    if text:
        linkedin += text

print(f"âœ… LinkedIn í”„ë¡œí•„ ì½ê¸° ì™„ë£Œ ({len(linkedin)} ê¸€ì)")

# Wanted PDF ì½ê¸° (ìˆë‹¤ë©´)
try:
    reader = PdfReader("../me/wanted.pdf")
    wanted = ""
    for page in reader.pages:
        text = page.extract_text()
        if text:
            wanted += text
    print(f"âœ… Wanted í”„ë¡œí•„ ì½ê¸° ì™„ë£Œ ({len(wanted)} ê¸€ì)")
except:
    wanted = ""
    print("âš ï¸  Wanted í”„ë¡œí•„ ì—†ìŒ (ì„ íƒì‚¬í•­)")

# Summary ì½ê¸°
with open("../me/summary.txt", "r", encoding="utf-8") as f:
    summary = f.read()

print(f"âœ… Summary ì½ê¸° ì™„ë£Œ ({len(summary)} ê¸€ì)")
```

`System Prompt ì„¤ì •`
```python

name = "bys"
print(f"âœ… ì´ë¦„ ì„¤ì •: {name}")

system_prompt = f"""You are acting as {name}. You are answering questions on {name}'s website, \
particularly questions related to {name}'s career, background, skills and experience. \
Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \
You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \
Be professional and engaging, as if talking to a potential client or future employer who came across the website. \
If you don't know the answer, say so."""

system_prompt += f"\n\n## Summary:\n{summary}\n\n"
system_prompt += f"## LinkedIn Profile:\n{linkedin}\n\n"
if wanted:
    system_prompt += f"## Wanted Profile:\n{wanted}\n\n"
system_prompt += f"With this context, please chat with the user, always staying in character as {name}."

print(f"âœ… System Prompt ìƒì„± ì™„ë£Œ ({len(system_prompt)} ê¸€ì)")
```


`Chroma DB ì´ˆê¸°í™”`  
```python
# ============================================
# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
# ============================================

import chromadb
from chromadb.utils import embedding_functions
import os
from tqdm import tqdm

print("ğŸš€ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")

# ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ë¡œì»¬ íŒŒì¼ë¡œ ì €ì¥)
chroma_client = chromadb.PersistentClient(path="./chroma_db")

# ì„ë² ë”© ëª¨ë¸ ì„¤ì • (all-MiniLM-L6-v2 ì‚¬ìš©)
embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name="all-MiniLM-L6-v2"  # 384ì°¨ì›, ë¹ ë¥´ê³  íš¨ìœ¨ì 
)

print("âœ… ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ")
print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (all-MiniLM-L6-v2)")
```


`Indexing í•¨ìˆ˜ ì •ì˜`  
```python
# ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì¸ë±ì‹± í•¨ìˆ˜
# ============================================

def index_markdown_files(rag_path, collection_name="work_cases"):
    """
    ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì½ì–´ì„œ ChromaDBì— ë²¡í„°ë¡œ ì €ì¥
    
    Args:
        rag_path: ì»¤ìŠ¤í…€ íŒŒì¼ì´ ìˆëŠ” ê²½ë¡œ
        collection_name: ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„
    
    Returns:
        collection: ChromaDB ì»¬ë ‰ì…˜ ê°ì²´
    """
    
    print(f"\nğŸ“ ê²½ë¡œ ìŠ¤ìº”: {rag_path}")
    
    # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ (ì¬ì¸ë±ì‹±)
    try:
        chroma_client.delete_collection(name=collection_name)
        print(f"âš ï¸  ê¸°ì¡´ ì»¬ë ‰ì…˜ '{collection_name}' ì‚­ì œ")
    except:
        pass
    
    # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
    collection = chroma_client.create_collection(
        name=collection_name,
        embedding_function=embedding_function,
        metadata={"description": "Work cases and project documentation"}
    )
    
    # ëª¨ë“  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì°¾ê¸°
    markdown_files = []
    for root, dirs, files in os.walk(rag_path):
        for file in files:
            if file.endswith('.md'):
                file_path = os.path.join(root, file)
                markdown_files.append(file_path)
    
    print(f"ğŸ“„ ë°œê²¬ëœ ë§ˆí¬ë‹¤ìš´ íŒŒì¼: {len(markdown_files)}ê°œ")
    
    if len(markdown_files) == 0:
        print("âš ï¸  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        print(f"ğŸ’¡ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {rag_path}")
        return collection
    
    # ê° íŒŒì¼ ì²˜ë¦¬
    documents = []
    metadatas = []
    ids = []
    doc_id = 0
    
    for file_path in tqdm(markdown_files, desc="ğŸ“ ì¸ë±ì‹± ì§„í–‰"):
        try:
            # íŒŒì¼ ì½ê¸°
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ë¹ˆ íŒŒì¼ ìŠ¤í‚µ
            if not content.strip():
                continue
            
            # ê¸´ íŒŒì¼ì€ ì²­í¬ë¡œ ë¶„í• 
            chunk_size = 1000  # 1000ìì”©
            overlap = 200      # 200ì ì˜¤ë²„ë© (ë¬¸ë§¥ ìœ ì§€)
            
            for i in range(0, len(content), chunk_size - overlap):
                chunk = content[i:i+chunk_size]
                
                if not chunk.strip():
                    continue
                
                # ë¬¸ì„œ ì¶”ê°€
                documents.append(chunk)
                metadatas.append({
                    "file_path": file_path,
                    "file_name": os.path.basename(file_path),
                    "chunk_id": i // (chunk_size - overlap),
                    "total_length": len(content)
                })
                ids.append(f"doc_{doc_id}")
                doc_id += 1
                
        except Exception as e:
            print(f"\nâš ï¸  íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {file_path} - {e}")
    
    # ChromaDBì— ë°°ì¹˜ ì¶”ê°€
    if documents:
        print(f"\nğŸ”„ {len(documents)}ê°œ ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ ì¤‘...")
        
        # ë°°ì¹˜ í¬ê¸°ë¡œ ë‚˜ëˆ ì„œ ì¶”ê°€ (ë©”ëª¨ë¦¬ íš¨ìœ¨)
        batch_size = 100
        for i in range(0, len(documents), batch_size):
            batch_docs = documents[i:i+batch_size]
            batch_metas = metadatas[i:i+batch_size]
            batch_ids = ids[i:i+batch_size]
            
            collection.add(
                documents=batch_docs,
                metadatas=batch_metas,
                ids=batch_ids
            )
        
        print(f"âœ… ì¸ë±ì‹± ì™„ë£Œ!")
        print(f"   ğŸ“ íŒŒì¼ ìˆ˜: {len(markdown_files)}")
        print(f"   ğŸ“¦ ì´ ì²­í¬: {len(documents)}")
        print(f"   ğŸ’¾ ì €ì¥ ìœ„ì¹˜: ./chroma_db")
    else:
        print("âš ï¸  ì¸ë±ì‹±í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤!")
    
    return collection

print("âœ… ì¸ë±ì‹± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")
```


`Indexing`  
ì‹¤ì œë¡œ 
path = "/Users/bys/workspace/work/cases"

```python
# ============================================
# Cases ë°ì´í„° ì¸ë±ì‹± ì‹¤í–‰
# ============================================

# ğŸ”¥ ì—¬ê¸°ë¥¼ ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”!
rag_path = "/Users/bys/workspace/work/cases"

# ê²½ë¡œ ì¡´ì¬ í™•ì¸
if os.path.exists(rag_path):
    print(f"âœ… ê²½ë¡œ í™•ì¸: {rag_path}")
    
    # ì¸ë±ì‹± ì‹¤í–‰
    collection = index_markdown_files(rag_path)
    
    # ê²°ê³¼ í™•ì¸
    count = collection.count()
    print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"   ì´ {count}ê°œì˜ ë²¡í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
else:
    print(f"âŒ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {rag_path}")
    print("ğŸ’¡ ê²½ë¡œë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
```



`ê²€ìƒ‰í•¨ìˆ˜ ì •ì˜`
```python
def search_relevant_cases(query, collection, top_k=3, min_similarity=0.6):
    """
    ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì¼€ì´ìŠ¤ ê²€ìƒ‰
    
    Args:
        query: ê²€ìƒ‰ ì§ˆë¬¸
        collection: ChromaDB ì»¬ë ‰ì…˜
        top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
        min_similarity: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.6 = 60%)
    
    Returns:
        relevant_content: í¬ë§·ëœ ê²€ìƒ‰ ê²°ê³¼ í…ìŠ¤íŠ¸
        search_results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    
    if collection is None:
        return "", []
    
    try:
        # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
        results = collection.query(
            query_texts=[query],
            n_results=top_k
        )
        
        # ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
        if not results['documents'][0]:
            return "", []
        
        # ê²°ê³¼ í¬ë§·íŒ…
        relevant_content = ""
        search_results = []
        
        for i, (doc, metadata, distance) in enumerate(zip(
            results['documents'][0],
            results['metadatas'][0],
            results['distances'][0]
        )):
            # ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
            similarity = 1 - distance
            
            # ìœ ì‚¬ë„ê°€ ì„ê³„ê°’ ì´ìƒì¸ ê²½ìš°ë§Œ í¬í•¨
            if similarity >= min_similarity:
                search_results.append({
                    'content': doc,
                    'file_name': metadata['file_name'],
                    'file_path': metadata['file_path'],
                    'chunk_id': metadata['chunk_id'],
                    'similarity': similarity
                })
                
                relevant_content += f"\n\n## ğŸ“Œ ê´€ë ¨ ì¼€ì´ìŠ¤ {len(search_results)} (ìœ ì‚¬ë„: {similarity:.2%})\n"
                relevant_content += f"**íŒŒì¼:** {metadata['file_name']}\n"
                relevant_content += f"**ì²­í¬:** {metadata['chunk_id']}\n\n"
                relevant_content += f"```\n{doc[:500]}{'...' if len(doc) > 500 else ''}\n```\n"
                relevant_content += "-" * 80
        
        return relevant_content, search_results
        
    except Exception as e:
        print(f"âš ï¸  ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return "", []

print("âœ… ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ (ìµœì†Œ ìœ ì‚¬ë„: 60%)")
```



`Chatbot í•¨ìˆ˜ ì •ì˜`
```python
def chat(message, history):
    """
    RAG ê¸°ëŠ¥ì´ í†µí•©ëœ ì±—ë´‡
    - ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì¼€ì´ìŠ¤ë¥¼ ChromaDBì—ì„œ ê²€ìƒ‰ (ìœ ì‚¬ë„ 60% ì´ìƒë§Œ ì‚¬ìš©)
    - ê²€ìƒ‰ ê²°ê³¼ë¥¼ System Promptì— ì¶”ê°€
    - AIê°€ ì‹¤ì œ ì¼€ì´ìŠ¤ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€ ìƒì„±
    """
    
    # 1. ê´€ë ¨ ì¼€ì´ìŠ¤ ê²€ìƒ‰ (ìœ ì‚¬ë„ 60% ì´ìƒ)
    print(f"\nğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{message}'")
    
    try:
        relevant_cases, search_results = search_relevant_cases(
            message, 
            collection, 
            top_k=3,
            min_similarity=0.5  # 60% ì´ìƒë§Œ ì‚¬ìš©
        )
        
        if search_results:
            print(f"âœ… {len(search_results)}ê°œ ê´€ë ¨ ì¼€ì´ìŠ¤ ë°œê²¬ (ìœ ì‚¬ë„ 60% ì´ìƒ):")
            for i, result in enumerate(search_results):
                print(f"   {i+1}. {result['file_name']} (ìœ ì‚¬ë„: {result['similarity']:.2%})")
        else:
            print("âš ï¸  ìœ ì‚¬ë„ 60% ì´ìƒì¸ ê´€ë ¨ ì¼€ì´ìŠ¤ ì—†ìŒ (ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€)")
    except Exception as e:
        print(f"âš ï¸  ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        relevant_cases = ""
        search_results = []
    
    # 2. System Prompt êµ¬ì„±
    if "patent" in message:
        system = system_prompt + "\n\nEverything in your reply needs to be in pig latin - \
              it is mandatory that you respond only and entirely in pig latin"
    else:
        system = system_prompt
    
    # 3. ê´€ë ¨ ì¼€ì´ìŠ¤ ì¶”ê°€ (RAGì˜ í•µì‹¬!)
    if relevant_cases:
        system += f"\n\n## ğŸ” ê´€ë ¨ ì‘ì—… ê²½í—˜ (ì‹¤ì œ ì¼€ì´ìŠ¤ from ChromaDB):\n{relevant_cases}\n"
        system += "\n**ì¤‘ìš”:** ìœ„ ì¼€ì´ìŠ¤ë“¤ì„ ì°¸ê³ í•˜ì—¬ êµ¬ì²´ì ì´ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. "
        system += "ì‹¤ì œ í”„ë¡œì íŠ¸ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ë˜, ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”.\n"
    
    # 4. ë©”ì‹œì§€ êµ¬ì„±
    messages = [
        {"role": "system", "content": system}
    ] + history + [
        {"role": "user", "content": message}
    ]
    
    # 5. AI ì‘ë‹µ ìƒì„±
    print("ğŸ¤– AI ì‘ë‹µ ìƒì„± ì¤‘...")
    response = openai.chat.completions.create(
        model=model_name,
        messages=messages
    )
    reply = response.choices[0].message.content
    
    print("âœ… ì‘ë‹µ ìƒì„± ì™„ë£Œ")
    return reply

print("âœ… RAG í†µí•© ì±—ë´‡ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")
```

`Chatbot ì‹¤í–‰ì „ ìµœì¢… ì ê²€`  
```python
print("ğŸ” RAG ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸...\n")

# 1. ChromaDB í´ë¼ì´ì–¸íŠ¸ í™•ì¸
try:
    print(f"âœ… ChromaDB í´ë¼ì´ì–¸íŠ¸: OK")
except NameError:
    print("âŒ ChromaDB í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤!")

# 2. ì»¬ë ‰ì…˜ í™•ì¸
try:
    if collection:
        count = collection.count()
        print(f"âœ… ì»¬ë ‰ì…˜ ë¡œë“œë¨: {count}ê°œ ë²¡í„°")
    else:
        print("âš ï¸  ì»¬ë ‰ì…˜ì´ Noneì…ë‹ˆë‹¤. initialize_rag.ipynbë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
except NameError:
    print("âŒ ì»¬ë ‰ì…˜ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

# 3. ê²€ìƒ‰ í•¨ìˆ˜ í™•ì¸
if collection:
    try:
        test_results = search_relevant_cases("í…ŒìŠ¤íŠ¸", collection, top_k=1)
        print(f"âœ… ê²€ìƒ‰ í•¨ìˆ˜ ì‘ë™ í™•ì¸")
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ í•¨ìˆ˜ ì˜¤ë¥˜: {e}")

print("\nğŸš€ ëª¨ë“  ì¤€ë¹„ ì™„ë£Œ! ì±—ë´‡ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
```


`Gradio ì±—ë´‡`   
```python
print("=" * 80)
print("ğŸ¤– RAG í†µí•© ì±—ë´‡ ì‹œì‘!")
print("=" * 80)
print("\nğŸ’¡ íŒ:")
print("   - ì§ˆë¬¸í•˜ë©´ ìë™ìœ¼ë¡œ ê´€ë ¨ ì¼€ì´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤")
print("   - ì½˜ì†”ì—ì„œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n")

# Gradio ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰
gr.ChatInterface(
    chat, 
    type="messages",
    title="ğŸ¤– RAG í†µí•© ê°œì¸ ì±—ë´‡",
    description=f"{name}ì˜ ì‹¤ì œ í”„ë¡œì íŠ¸ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” AI ì±—ë´‡",
    examples=[
        "EKS íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê²½í—˜ì´ ìˆë‚˜ìš”?",
        "CI/CD íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•œ ê²½í—˜ì„ ë§í•´ì£¼ì„¸ìš”",
        "AWS ì•„í‚¤í…ì²˜ ì„¤ê³„ ê²½í—˜ì€?",
        "Kubernetes ë°°í¬ ìë™í™” ê²½í—˜"
    ]
).launch()
```

---

ìœ„ ì½”ë“œë“¤ì„ ì‹¤í–‰í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ Gradio ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ ë‚˜ì˜ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆëŠ” LLMê³¼ ëŒ€í™”ë¥¼ í•  ìˆ˜ ìˆë‹¤. ì •ë³´ë“¤ì´ ì—„ì²­ ì •í™•í•˜ë‹¤ê³  í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ ê¸°ë³¸ì ì¸ RAG ì•„í‚¤í…ì²˜ì™€ ì»¨ì…‰ì„ í†µí•´ ë‚˜ì˜ ì •ë³´ì— ëŒ€í•´ ì–´ëŠì •ë„ ì •í™•íˆ ë¶„ì„í•˜ì—¬ ë‹µë³€ì„ í•´ì£¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.  

![test1.png](/assets/it/ml/rag/test1.png)  

![test2.png](/assets/it/ml/rag/test2.png)  

![test3.png](/assets/it/ml/rag/test3.png)  

![test4.png](/assets/it/ml/rag/test4.png)  

![test5.png](/assets/it/ml/rag/test5.png)  

---

## ğŸ“š References

[1] **What is Retrieval-Augmented Generation?**
- https://aws.amazon.com/what-is/retrieval-augmented-generation/  

[2] How Amazon Bedrock knowledge bases work
- https://docs.aws.amazon.com/bedrock/latest/userguide/kb-how-it-works.html

[3] **Retrieval Augmented Generation - IBM**
- https://www.ibm.com/architectures/patterns/genai-rag
